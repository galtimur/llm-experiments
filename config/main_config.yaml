general:
  s3_bucket: 'jettrain-experiments'
  wandb_project: 'llm-experiments'
  num_workers: 1
model:
  sequence_length: 512
  name: "deepseek-ai/deepseek-coder-1.3b-base" # "codellama/CodeLlama-7b-hf"
train:
  num_epochs: 1
  train_mini_batch_size: 4
  train_batch_size: 128
  val_batch_size: 4
  num_gpus: 8
  precision: "bf16"
  # device: 'cuda:1'
  # steps|samples counters
  # Prioritization: samples-steps-ratio
  # samples = steps*full_batch_size
  steps:
    warmup_samples: 4000
    warmup_steps: 1000
    warmup_ratio: 0.01
    val_every_sample: 8000
    val_every_step: 2000
    max_val_samples: 10000
    max_val_steps: 2500
    max_train_samples: 1000000
    max_train_steps: 250000
    save_every_sample: 20000
    save_every_step: 5000
  # Optimizer settings
  max_lr: 0.0001
  weight_decay: 0.00
  max_norm: 10000
  scheduler: 'cosine'
  # Save settings
  upload_to_s3: False
  s3_checkpoint_path:
  model_checkpoints: 'mnt/data2/galimzyanov/llm-experiments/gpt2-zip-compression/'
data:
  train_name_or_path: "openwebtext"
  train_ds_subset: null
  val_name_or_path: "wikitext"
  val_ds_subset: "wikitext-103-raw-v1"
  text_key: "text"
  num_workers: 0
  save_path: 'mnt/data2/galimzyanov/datasets/'
  validation_path:
  s3_path:
  valid_portion: 0.01
