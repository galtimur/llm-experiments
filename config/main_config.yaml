general:
  s3_bucket: 'jettrain-experiments'
  wandb_url: "https://api.wandb.ai"
  wandb_entity: 'timur-galimzyanov'
  wandb_project: 'llm-experiments'
  num_workers: 1
  checkpoints_path: '/mnt/data/galimzyanov/llm-experiments/gpt2-zip-compression/'
  upload_to_s3: False
model:
  sequence_length: 256
  name: "TinyLlama/TinyLlama_v1.1"
  # "meta-llama/Meta-Llama-3.1-8B" # "HuggingFaceTB/SmolLM-135M"# "deepseek-ai/deepseek-coder-1.3b-base"
  # "codellama/CodeLlama-7b-hf", "TinyLlama/TinyLlama_v1.1", "openai-community/gpt2"
  pretrained: True
  use_flash_attn: true
  parameters:
    num_layers: 16
    hidden_dim: 1024
    ff_dim: 4096
train:
  train_mini_batch_size: 1
  train_batch_size: 8
  val_batch_size: 16
  num_gpus: 8
  precision: "fp32" # "bf16", "fp32"
  use_flash_attn: false
  lora:
    lora: false
  # device: 'cuda:1'
  # steps|samples counters
  # Prioritization: samples-steps-ratio
  # samples = steps*full_batch_size
  steps:
    num_epochs: 1
    max_train_samples: 500_000_000
    max_train_steps: 250_000
    warmup_samples: 40000
    warmup_steps_or_ratio: 1000
    val_every_sample: null
    val_every_step: 2000
    max_val_samples: 20000
    max_val_steps: 5000
    save_every_sample: null
    save_every_step: 2000
  # Optimizer settings
  max_lr: 0.0001
  weight_decay: 0.00
  max_norm: 10000
  scheduler: 'cosine'
  do_sanity_check: false
data:
  train_name_or_path: "openwebtext"
  train_ds_subset: null
  val_name_or_path: "wikitext"
  val_ds_subset: "wikitext-103-raw-v1"
  text_key: "text"
  num_workers: 0
  processed_folder: '/mnt/data2/galimzyanov/llm-experiments/data/'
