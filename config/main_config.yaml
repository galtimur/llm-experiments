general:
  s3_bucket: 'jettrain-experiments'
  wandb_project: 'llm-experiments'
  num_workers: 1
  checkpoints_path: '/mnt/data2/galimzyanov/llm-experiments/gpt2-zip-compression/'
  upload_to_s3: False
model:
  sequence_length: 512
  name: "HuggingFaceTB/SmolLM-135M"# "deepseek-ai/deepseek-coder-1.3b-base" # "codellama/CodeLlama-7b-hf"
  pretrained: true
  use_flash_attn: true
train:
  train_mini_batch_size: 2
  train_batch_size: 8
  val_batch_size: 4
  num_gpus: 8
  precision: "bf16"
  lora:
    lora: false
  # device: 'cuda:1'
  # steps|samples counters
  # Prioritization: samples-steps-ratio
  # samples = steps*full_batch_size
  steps:
    num_epochs: 1
    max_train_samples: 1_000_000
    max_train_steps: 250_000
    warmup_samples: 4000
    warmup_steps_or_ratio: 1000
    val_every_sample: 8000
    val_every_step: 2000
    max_val_samples: 10000
    max_val_steps: 2500
    save_every_sample: 20_000
    save_every_step: 5000
  # Optimizer settings
  max_lr: 0.0001
  weight_decay: 0.00
  max_norm: 10000
  scheduler: 'cosine'
  do_sanity_check: false
data:
  train_name_or_path: "openwebtext"
  train_ds_subset: null
  val_name_or_path: "wikitext"
  val_ds_subset: "wikitext-103-raw-v1"
  text_key: "text"
  num_workers: 0
  processed_folder: '/mnt/data2/galimzyanov/llm-experiments/data/'
