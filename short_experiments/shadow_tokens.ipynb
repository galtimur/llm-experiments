{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T18:09:37.175516Z",
     "start_time": "2025-03-21T18:09:37.171530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from emb_vectors_functions import find_self_embeds, get_shadow_ratios\n",
    "from model_loading import get_weight_by_name"
   ],
   "id": "c8a4c2d3a58c68cd",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:52.557082Z",
     "start_time": "2025-03-21T19:23:48.565406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# \"meta-llama/Llama-3.1-70B\"\n",
    "# \"Qwen/Qwen2.5-0.5B-Instruct\", \"Qwen/Qwen2.5-1.5B-Instruct\", \"Qwen/Qwen2.5-3B-Instruct\", \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\", \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embeddings = get_weight_by_name(model_name, \"head\")\n",
    "embeddings = embeddings.cuda()"
   ],
   "id": "a2224e6661df7de4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c18aeca2da5c4742acbddfbd85a41087"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head was not found. Trying to load the full model.\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:53.570051Z",
     "start_time": "2025-03-21T19:23:53.567603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# embeddings = torch.randn_like(embeddings)\n",
    "embeddings.requires_grad = False"
   ],
   "id": "d4805a55975d91ef",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:56.485279Z",
     "start_time": "2025-03-21T19:23:53.787157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fail_indices, failed_res_emb, failed_pairs = find_self_embeds(embeddings, tokenizer)\n",
    "print(f\"Number of bad embeddings = {len(fail_indices)}\")"
   ],
   "id": "35320c67765020f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad embeddings = 8913\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:56.530627Z",
     "start_time": "2025-03-21T19:23:56.528248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# shadow_ratios = get_shadow_ratios(fail_indices, embeddings)\n",
    "# shadow_ratios_sorted = sorted(shadow_ratios, key=lambda x: x[1], reverse=True);"
   ],
   "id": "e8946f32374c8154",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:57.125691Z",
     "start_time": "2025-03-21T19:23:56.570342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_list(lst, batch_size):\n",
    "    return [lst[i : i + batch_size] for i in range(0, len(lst), batch_size)]\n",
    "\n",
    "\n",
    "def calc_loss(x, self_emb, X, mask, epsilon=1e-4):\n",
    "\n",
    "    xself = torch.einsum(\"ij,ij->i\", x, self_emb)\n",
    "    xX = x @ X.T\n",
    "\n",
    "    xA = xself[:, None] - xX\n",
    "    xA = xA * mask\n",
    "    loss = torch.sum(torch.relu(-xA + epsilon))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_bad_embeds(x_optim, self_emb, embeddings, mask):\n",
    "\n",
    "    xself = torch.einsum(\"ij,ij->i\", x_optim, self_emb)\n",
    "    xX = x_optim @ embeddings.T\n",
    "    xA = xself[:, None] - xX\n",
    "    xA = xA + 1e10 * (1 - mask)\n",
    "    is_good_embed = torch.all(xA > 0, dim=1)\n",
    "    bad_embeds = len(is_good_embed) - sum(is_good_embed).item()\n",
    "    bad_embeds_ratio = bad_embeds / len(is_good_embed)\n",
    "\n",
    "    return bad_embeds, bad_embeds_ratio, np.array(is_good_embed.cpu())\n",
    "\n",
    "\n",
    "def train_vectors(n_lst, embeddings, x_optim_start=None, n_steps=100, verbose=False, use_tqdm=True):\n",
    "    min_bad = len(n_lst)\n",
    "    X = embeddings\n",
    "    self_emb = X[n_lst]\n",
    "    mask = torch.ones((len(n_lst), len(X)), requires_grad=False, device=X.device)\n",
    "    indices = torch.arange(len(n_lst))\n",
    "    mask[indices, n_lst] = 0\n",
    "\n",
    "    if x_optim_start is None:\n",
    "        x_optim = self_emb.detach().clone()\n",
    "    else:\n",
    "        x_optim = x_optim_start.detach().clone()\n",
    "    x_optim.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW([x_optim], lr=0.01)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        bad_embeds, bad_embeds_ratio, is_good_embed = calc_bad_embeds(\n",
    "            x_optim, self_emb, embeddings, mask\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"Initial\\nloss = {loss.item()}\")\n",
    "            print(f\"Bad embeds = {bad_embeds}/{len(n_lst)}, ratio = {bad_embeds_ratio}\")\n",
    "\n",
    "    pbar = tqdm(range(n_steps), disable=not use_tqdm)\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (step + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                bad_embeds, bad_embeds_ratio, is_good_embed = calc_bad_embeds(\n",
    "                    x_optim, self_emb, embeddings, mask\n",
    "                )\n",
    "            pbar.set_postfix_str(f\"Bad embeds: {bad_embeds}/{len(n_lst)}\")\n",
    "            if bad_embeds < min_bad:\n",
    "                min_bad = bad_embeds\n",
    "            if bad_embeds_ratio == 0.0:\n",
    "                break\n",
    "            # if verbose:\n",
    "            #     print(f\"Step {step + 1}, Loss: {loss.item()}, bad embeds = {bad_embeds}/{len(n_lst)}, ratio = {bad_embeds_ratio}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        bad_embeds, bad_embeds_ratio, is_good_embed = calc_bad_embeds(\n",
    "            x_optim, self_emb, embeddings, mask\n",
    "        )\n",
    "    if verbose:\n",
    "        print(\"Final\")\n",
    "        print(f\"steps = {step+1}, loss = {loss.item()}\")\n",
    "        print(\n",
    "            f\"Bad embeds = {bad_embeds}/{len(n_lst)}, ratio = {bad_embeds_ratio:.4f}, Minimal bad = {min_bad}\"\n",
    "        )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return loss, x_optim, self_emb, mask, is_good_embed"
   ],
   "id": "56ced80957683661",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:57.499356Z",
     "start_time": "2025-03-21T19:23:57.159378Z"
    }
   },
   "cell_type": "code",
   "source": "n_lst = fail_indices",
   "id": "7d8304977669069b",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:57.940388Z",
     "start_time": "2025-03-21T19:23:57.543286Z"
    }
   },
   "cell_type": "code",
   "source": "# loss, x_optim, self_emb, mask, bad_indices = train_vectors(n_lst, embeddings, n_steps=100, verbose=True)",
   "id": "7e0ac16307e2f75c",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:58.218248Z",
     "start_time": "2025-03-21T19:23:57.985646Z"
    }
   },
   "cell_type": "code",
   "source": "ind_batched_list = batch_list(n_lst, 1000)",
   "id": "c609f03ce834233f",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:26:04.086476Z",
     "start_time": "2025-03-21T19:23:58.264120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Number of bad embeddings = {len(n_lst)}, dimension = {embeddings.shape[1]}, dict size = {embeddings.shape[0]}\")\n",
    "\n",
    "for ind_list_ in ind_batched_list:\n",
    "    x_optim = None\n",
    "    ind_list = ind_list_\n",
    "    pbar = tqdm(range(100))\n",
    "    for i in pbar:\n",
    "        loss, x_optim, self_emb, mask, is_good_embed = train_vectors(\n",
    "            ind_list,\n",
    "            embeddings,\n",
    "            x_optim_start=x_optim,\n",
    "            n_steps=100,\n",
    "            verbose=False,\n",
    "            use_tqdm=False,\n",
    "        )\n",
    "        ind_list = np.array(ind_list)[~is_good_embed]\n",
    "        x_optim = x_optim[~is_good_embed]\n",
    "        pbar.set_postfix_str(f\"Bad embeds: {len(ind_list)}/{len(ind_list_)}\")\n",
    "\n",
    "        if len(ind_list) == 0:\n",
    "            break"
   ],
   "id": "6ada7969a4043995",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad embeddings = 8913, dimension = 896, dict size = 151936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:05<01:26,  1.08it/s, Bad embeds: 0/1000]  \n",
      " 33%|███▎      | 33/100 [00:20<00:40,  1.64it/s, Bad embeds: 0/1000] \n",
      " 21%|██        | 21/100 [00:14<00:54,  1.45it/s, Bad embeds: 0/1000] \n",
      " 22%|██▏       | 22/100 [00:14<00:50,  1.53it/s, Bad embeds: 0/1000] \n",
      " 71%|███████   | 71/100 [00:30<00:12,  2.36it/s, Bad embeds: 0/1000] \n",
      " 11%|█         | 11/100 [00:07<00:58,  1.51it/s, Bad embeds: 0/1000]\n",
      " 27%|██▋       | 27/100 [00:13<00:35,  2.03it/s, Bad embeds: 0/1000]\n",
      "  4%|▍         | 4/100 [00:05<02:00,  1.25s/it, Bad embeds: 0/1000]  \n",
      " 27%|██▋       | 27/100 [00:15<00:41,  1.75it/s, Bad embeds: 0/913] \n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T19:23:39.180232Z",
     "start_time": "2025-03-21T19:23:39.176955Z"
    }
   },
   "cell_type": "code",
   "source": "1+1",
   "id": "c0740d763534964d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "51ec90ffd287d4ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "267236dd01d5db5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
