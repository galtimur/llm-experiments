{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:12:09.600514Z",
     "start_time": "2025-03-18T19:12:09.596676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from emb_vectors_functions import (\n",
    "    find_self_embeds,\n",
    "    get_model_and_embed,\n",
    "    get_shadow_ratios,\n",
    ")"
   ],
   "id": "c8a4c2d3a58c68cd",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:29:42.300730Z",
     "start_time": "2025-03-18T18:29:39.351892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"  # \"meta-llama/Meta-Llama-3.1-8B\"  # \"meta-llama/Meta-Llama-3.1-8B\" , \"gpt2\", \"meta-llama/Llama-2-7b-hf\"\n",
    "model, embedding, head, model_norm, mean_norm, tokenizer = get_model_and_embed(\n",
    "    model_name\n",
    ")\n",
    "\n",
    "del model, embedding\n",
    "embeddings = head\n",
    "embeddings.requires_grad = False"
   ],
   "id": "d4805a55975d91ef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:29:45.189931Z",
     "start_time": "2025-03-18T18:29:42.513562Z"
    }
   },
   "cell_type": "code",
   "source": "fail_indices, failed_res_emb, failed_pairs = find_self_embeds(embeddings, tokenizer)",
   "id": "35320c67765020f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  6.31it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:29:45.964616Z",
     "start_time": "2025-03-18T18:29:45.195705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shadow_ratios = get_shadow_ratios(fail_indices, embeddings)\n",
    "shadow_ratios_sorted = sorted(shadow_ratios, key=lambda x: x[1], reverse=True);"
   ],
   "id": "e8946f32374c8154",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:12:20.124961Z",
     "start_time": "2025-03-18T19:12:20.120840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# n = 124\n",
    "# embeddings_cut = embeddings  # embeddings[:10000]\n",
    "# embeddings_others = torch.cat((embeddings_cut[:n], embeddings_cut[n + 1 :]), dim=0)\n",
    "# self_emb = embeddings_cut[n]\n",
    "#\n",
    "# A = self_emb - embeddings_others\n",
    "# A.requires_grad = False\n",
    "# torch.all(A @ self_emb > 0)\n",
    "#\n",
    "# embeddings_others = embeddings_others.detach()\n",
    "# self_emb = self_emb.detach()\n",
    "# A = A.detach()\n",
    "# x = self_emb.detach().clone()\n",
    "# x.requires_grad = True\n",
    "# optimizer = torch.optim.SGD([x], lr=0.01)\n",
    "# optimizer = torch.optim.AdamW([x], lr=0.01)\n",
    "#\n",
    "# epsilon = 1e-4\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     loss = torch.sum(torch.relu(-A @ x + epsilon))\n",
    "#     print(f\"Initial loss = {loss}\")\n",
    "#\n",
    "# for step in range(1000):\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = torch.sum(torch.relu(-A @ x + epsilon))\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#\n",
    "#     if (step + 1) % 100 == 0:\n",
    "#         print(f\"Step {step+1}, Loss: {loss.item()}\")\n",
    "#     if loss.item() <= 0:\n",
    "#         break\n",
    "#\n",
    "# print(\"Optimization finished. Final x:\", loss.item())\n",
    "# print(f\"Target embedding - {torch.all(A@x>0).item()}, min = {torch.min(A@x)}\")"
   ],
   "id": "625b514938fed3e8",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:14:00.058237Z",
     "start_time": "2025-03-18T19:14:00.049679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_loss(x, self_emb, X, mask, epsilon = 1e-4):\n",
    "\n",
    "    xself = torch.einsum('ij,ij->i', x, self_emb)\n",
    "    xX = x@X.T\n",
    "\n",
    "    xA = xself[:, None]-xX\n",
    "    xA = xA*mask\n",
    "    loss = torch.sum(torch.relu(-xA + epsilon))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_vectors(n_lst, embeddings, n_steps=100, verbose=False):\n",
    "    X = embeddings\n",
    "    self_emb = X[n_lst]\n",
    "    mask = torch.ones((len(n_lst), len(X)), requires_grad=False, device=X.device)\n",
    "    indices = torch.arange(len(n_lst))\n",
    "    mask[indices, n_lst] = 0\n",
    "\n",
    "    x_optim = self_emb.detach().clone()\n",
    "    x_optim.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW([x_optim], lr=0.01)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        print(f\"Initial loss = {loss.item()}\")\n",
    "\n",
    "    for step in tqdm(range(n_steps)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose and (step+1) % 100 == 0:\n",
    "            print(f\"Step {step + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        print(f\"Final loss = {loss.item()}\")\n",
    "\n",
    "    return loss, x_optim, self_emb, mask"
   ],
   "id": "56ced80957683661",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:14:01.507183Z",
     "start_time": "2025-03-18T19:14:01.503636Z"
    }
   },
   "cell_type": "code",
   "source": "n_lst = fail_indices",
   "id": "7d8304977669069b",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:15:08.930851Z",
     "start_time": "2025-03-18T19:14:49.334377Z"
    }
   },
   "cell_type": "code",
   "source": "loss, x_optim, self_emb, mask = train_vectors(n_lst, embeddings, n_steps=1000);",
   "id": "7e0ac16307e2f75c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss = 3053.449951171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 52.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss = 0.06469347327947617\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:15:10.337291Z",
     "start_time": "2025-03-18T19:15:10.333236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xself = torch.einsum('ij,ij->i', x_optim, self_emb)\n",
    "xX = x_optim@embeddings.T\n",
    "xA = xself[:, None]-xX\n",
    "xA1 = xA + 1e10*(1-mask)"
   ],
   "id": "894e110bb4b731c5",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:15:11.786821Z",
     "start_time": "2025-03-18T19:15:11.779982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xqq = torch.min(xA1, dim=1)[0]\n",
    "torch.min(xqq)"
   ],
   "id": "a9904b13f41d051d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3809e-06, device='cuda:0', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:15:21.230533Z",
     "start_time": "2025-03-18T19:15:17.677453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qq = []\n",
    "\n",
    "for n, x in zip(n_lst, x_optim):\n",
    "    embeddings_others = torch.cat((embeddings[:n], embeddings[n + 1 :]), dim=0)\n",
    "    x0 = embeddings[n]\n",
    "    A = x0 - embeddings_others\n",
    "    # print(f\"Target embedding - {torch.all(A@x>0).item()}, min = {torch.min(A@x)}\")\n",
    "    qq.append(torch.all(A@x>0).item())"
   ],
   "id": "6e45092a02b91d37",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:15:22.671431Z",
     "start_time": "2025-03-18T19:15:22.667852Z"
    }
   },
   "cell_type": "code",
   "source": "sum(qq)/len(qq)",
   "id": "2567a0c7a451d356",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:08:16.232087Z",
     "start_time": "2025-03-18T19:08:16.228460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    # if loss.item() <= 0:\n",
    "    #     break\n",
    "\n",
    "# print(\"Optimization finished. Final loss:\", loss.item())\n",
    "#\n",
    "# # Check final condition for each embedding in X\n",
    "# with torch.no_grad():\n",
    "#     target_embeddings_check = torch.all(torch.matmul(A, x_init.unsqueeze(2)).squeeze(2) > 0, dim=1)\n",
    "#     min_values = torch.min(torch.matmul(A, x_init.unsqueeze(2)).squeeze(2), dim=1).values\n",
    "#\n",
    "# print(f\"Target embeddings satisfied for all n? - {torch.all(target_embeddings_check).item()}\")\n",
    "# print(f\"Minimum values per embedding: {min_values}\")"
   ],
   "id": "8bbf5d845ff6c6dc",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae3541425728e7fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T20:57:52.884206Z",
     "start_time": "2025-03-17T20:57:52.881112Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "452b400be54728ae",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cc57ca631a01cc51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:48:25.790358Z",
     "start_time": "2025-03-18T17:48:25.783445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n, d = 5, 6  # Example sizes\n",
    "self_emb = torch.randn(n, d)\n",
    "n_lst = [1, 3, 0, 4, 2]\n",
    "\n",
    "mask = torch.ones_like(self_emb, requires_grad=False)\n",
    "\n",
    "indices = torch.arange(len(n_lst))  # [0, 1, 2, 3, 4]\n",
    "mask[indices, n_lst] = 0\n",
    "\n",
    "print(\"Generated mask:\")\n",
    "print(mask)"
   ],
   "id": "f8854fb64ce3c076",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated mask:\n",
      "tensor([[1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "331ee4867924811d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
