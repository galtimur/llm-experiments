{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:29:39.344917Z",
     "start_time": "2025-03-18T18:29:35.920638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from emb_vectors_functions import (\n",
    "    find_self_embeds,\n",
    "    get_model_and_embed,\n",
    "    get_shadow_ratios,\n",
    ")"
   ],
   "id": "c8a4c2d3a58c68cd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:29:42.300730Z",
     "start_time": "2025-03-18T18:29:39.351892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"  # \"meta-llama/Meta-Llama-3.1-8B\"  # \"meta-llama/Meta-Llama-3.1-8B\" , \"gpt2\", \"meta-llama/Llama-2-7b-hf\"\n",
    "model, embedding, head, model_norm, mean_norm, tokenizer = get_model_and_embed(\n",
    "    model_name\n",
    ")\n",
    "\n",
    "del model, embedding\n",
    "embeddings = head\n",
    "embeddings.requires_grad = False"
   ],
   "id": "d4805a55975d91ef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:29:45.189931Z",
     "start_time": "2025-03-18T18:29:42.513562Z"
    }
   },
   "cell_type": "code",
   "source": "fail_indices, failed_res_emb, failed_pairs = find_self_embeds(embeddings, tokenizer)",
   "id": "35320c67765020f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  6.31it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:29:45.964616Z",
     "start_time": "2025-03-18T18:29:45.195705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shadow_ratios = get_shadow_ratios(fail_indices, embeddings)\n",
    "shadow_ratios_sorted = sorted(shadow_ratios, key=lambda x: x[1], reverse=True);"
   ],
   "id": "e8946f32374c8154",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:55:48.125499Z",
     "start_time": "2025-03-18T18:55:43.950992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = 124\n",
    "embeddings_cut = embeddings  # embeddings[:10000]\n",
    "embeddings_others = torch.cat((embeddings_cut[:n], embeddings_cut[n + 1 :]), dim=0)\n",
    "self_emb = embeddings_cut[n]\n",
    "\n",
    "A = self_emb - embeddings_others\n",
    "A.requires_grad = False\n",
    "torch.all(A @ self_emb > 0)\n",
    "\n",
    "embeddings_others = embeddings_others.detach()\n",
    "self_emb = self_emb.detach()\n",
    "A = A.detach()\n",
    "x = self_emb.detach().clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.SGD([x], lr=0.01)\n",
    "optimizer = torch.optim.AdamW([x], lr=0.01)\n",
    "\n",
    "epsilon = 1e-4\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = torch.sum(torch.relu(-A @ x + epsilon))\n",
    "    print(f\"Initial loss = {loss}\")\n",
    "\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.sum(torch.relu(-A @ x + epsilon))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"Step {step+1}, Loss: {loss.item()}\")\n",
    "    if loss.item() <= 0:\n",
    "        break\n",
    "\n",
    "print(\"Optimization finished. Final x:\", loss.item())\n",
    "print(f\"Target embedding - {torch.all(A@x>0).item()}, min = {torch.min(A@x)}\")"
   ],
   "id": "625b514938fed3e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss = 6.937034606933594\n",
      "Step 100, Loss: 0.00045663019409403205\n",
      "Step 200, Loss: 0.0001864949445007369\n",
      "Step 300, Loss: 0.00011208559590158984\n",
      "Step 400, Loss: 7.662771531613544e-05\n",
      "Step 500, Loss: 6.027078052284196e-05\n",
      "Step 600, Loss: 3.7434889236465096e-05\n",
      "Step 700, Loss: 1.3979071809444577e-05\n",
      "Step 800, Loss: 4.358837031759322e-06\n",
      "Step 900, Loss: 0.0\n",
      "Optimization finished. Final x: 0.0\n",
      "Target embedding - True, min = 0.00010008066601585597\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:53:50.703712Z",
     "start_time": "2025-03-18T18:53:50.696598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_loss(x, self_emb, X, mask, epsilon = 1e-4):\n",
    "\n",
    "    xself = torch.einsum('ij,ij->i', x, self_emb)\n",
    "    xX = x@X.T\n",
    "\n",
    "    xA = xself[:, None]-xX\n",
    "    xA = xA*mask\n",
    "    loss = torch.sum(torch.relu(-xA + epsilon))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_vectors(n_lst, embeddings, n_steps=100, verbose=False):\n",
    "    X = embeddings\n",
    "    self_emb = X[n_lst]\n",
    "    mask = torch.ones((len(n_lst), len(X)), requires_grad=False, device=X.device)\n",
    "    indices = torch.arange(len(n_lst))\n",
    "    mask[indices, n_lst] = 0\n",
    "\n",
    "    x_optim = self_emb.detach().clone()\n",
    "    x_optim.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW([x_optim], lr=0.01)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        print(f\"Initial loss = {loss.item()}\")\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose and (step+1) % 100 == 0:\n",
    "            print(f\"Step {step + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        print(f\"Final loss = {loss.item()}\")\n",
    "\n",
    "    return loss, x_optim, self_emb, mask"
   ],
   "id": "56ced80957683661",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:53:51.037303Z",
     "start_time": "2025-03-18T18:53:51.034111Z"
    }
   },
   "cell_type": "code",
   "source": "n_lst = fail_indices[:4]",
   "id": "7d8304977669069b",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:53:53.751395Z",
     "start_time": "2025-03-18T18:53:51.376142Z"
    }
   },
   "cell_type": "code",
   "source": "loss, x_optim, self_emb, mask = train_vectors(n_lst, embeddings, n_steps=1000);",
   "id": "7e0ac16307e2f75c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss = 27.730876922607422\n",
      "Final loss = 0.0004165483987890184\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T18:53:53.759985Z",
     "start_time": "2025-03-18T18:53:53.756936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xself = torch.einsum('ij,ij->i', x_optim, self_emb)\n",
    "xX = x_optim@embeddings.T\n",
    "xA = xself[:, None]-xX\n",
    "xA1 = xA + 1e10*(1-mask)"
   ],
   "id": "894e110bb4b731c5",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:04:46.694094Z",
     "start_time": "2025-03-18T19:04:46.687480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xqq = torch.min(xA1, dim=1)[0]\n",
    "xqq"
   ],
   "id": "a9904b13f41d051d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.3132e-05, 9.7319e-05, 1.0000e-04, 9.3073e-05], device='cuda:0',\n",
       "       grad_fn=<MinBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:10:52.700380Z",
     "start_time": "2025-03-18T19:10:52.667076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n, x in zip(n_lst, x_optim):\n",
    "    embeddings_others = torch.cat((embeddings[:n], embeddings[n + 1 :]), dim=0)\n",
    "    x0 = embeddings[n]\n",
    "    A = x0 - embeddings_others\n",
    "    print(f\"Target embedding - {torch.all(A@x>0).item()}, min = {torch.min(A@x)}\")"
   ],
   "id": "6e45092a02b91d37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target embedding - True, min = 9.319287346443161e-05\n",
      "Target embedding - True, min = 9.72848865785636e-05\n",
      "Target embedding - True, min = 0.00010005751391872764\n",
      "Target embedding - True, min = 9.309838060289621e-05\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:08:16.232087Z",
     "start_time": "2025-03-18T19:08:16.228460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    # if loss.item() <= 0:\n",
    "    #     break\n",
    "\n",
    "# print(\"Optimization finished. Final loss:\", loss.item())\n",
    "#\n",
    "# # Check final condition for each embedding in X\n",
    "# with torch.no_grad():\n",
    "#     target_embeddings_check = torch.all(torch.matmul(A, x_init.unsqueeze(2)).squeeze(2) > 0, dim=1)\n",
    "#     min_values = torch.min(torch.matmul(A, x_init.unsqueeze(2)).squeeze(2), dim=1).values\n",
    "#\n",
    "# print(f\"Target embeddings satisfied for all n? - {torch.all(target_embeddings_check).item()}\")\n",
    "# print(f\"Minimum values per embedding: {min_values}\")"
   ],
   "id": "8bbf5d845ff6c6dc",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae3541425728e7fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T20:57:52.884206Z",
     "start_time": "2025-03-17T20:57:52.881112Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "452b400be54728ae",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cc57ca631a01cc51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:48:25.790358Z",
     "start_time": "2025-03-18T17:48:25.783445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n, d = 5, 6  # Example sizes\n",
    "self_emb = torch.randn(n, d)\n",
    "n_lst = [1, 3, 0, 4, 2]\n",
    "\n",
    "mask = torch.ones_like(self_emb, requires_grad=False)\n",
    "\n",
    "indices = torch.arange(len(n_lst))  # [0, 1, 2, 3, 4]\n",
    "mask[indices, n_lst] = 0\n",
    "\n",
    "print(\"Generated mask:\")\n",
    "print(mask)"
   ],
   "id": "f8854fb64ce3c076",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated mask:\n",
      "tensor([[1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "331ee4867924811d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
