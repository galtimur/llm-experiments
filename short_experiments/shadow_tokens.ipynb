{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:36:58.384925Z",
     "start_time": "2025-03-21T07:36:58.381884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from emb_vectors_functions import find_self_embeds, get_shadow_ratios\n",
    "from model_loading import get_weight_by_name\n"
   ],
   "id": "c8a4c2d3a58c68cd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:35:58.327739Z",
     "start_time": "2025-03-21T07:35:57.180869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"meta-llama/Llama-3.1-70B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embeddings = get_weight_by_name(model_name, \"head\")\n",
    "embeddings = embeddings.cuda()"
   ],
   "id": "a2224e6661df7de4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 50 files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f093548100da402ba8fd62c27dd94a2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:36:05.404818Z",
     "start_time": "2025-03-21T07:36:05.401312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# embeddings = torch.randn_like(embeddings)\n",
    "embeddings.requires_grad = False"
   ],
   "id": "d4805a55975d91ef",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:36:10.733333Z",
     "start_time": "2025-03-21T07:36:08.079273Z"
    }
   },
   "cell_type": "code",
   "source": "fail_indices, failed_res_emb, failed_pairs = find_self_embeds(embeddings, tokenizer)",
   "id": "35320c67765020f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  6.94it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:37:05.465683Z",
     "start_time": "2025-03-21T07:37:05.306967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "12b94ea69675ae58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:37:11.871844Z",
     "start_time": "2025-03-21T07:37:09.384273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shadow_ratios = get_shadow_ratios(fail_indices, embeddings)\n",
    "shadow_ratios_sorted = sorted(shadow_ratios, key=lambda x: x[1], reverse=True);"
   ],
   "id": "e8946f32374c8154",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:38:42.505022Z",
     "start_time": "2025-03-21T08:38:42.497193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_loss(x, self_emb, X, mask, epsilon = 1e-4):\n",
    "\n",
    "    xself = torch.einsum('ij,ij->i', x, self_emb)\n",
    "    xX = x@X.T\n",
    "\n",
    "    xA = xself[:, None]-xX\n",
    "    xA = xA*mask\n",
    "    loss = torch.sum(torch.relu(-xA + epsilon))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def calc_bad_embeds(x_optim, self_emb, embeddings, mask):\n",
    "\n",
    "    xself = torch.einsum('ij,ij->i', x_optim, self_emb)\n",
    "    xX = x_optim@embeddings.T\n",
    "    xA = xself[:, None]-xX\n",
    "    xA = xA + 1e10*(1-mask)\n",
    "    is_good_embed = torch.all(xA>0, dim=1)\n",
    "    bad_embeds = len(is_good_embed)-sum(is_good_embed).item()\n",
    "    bad_embeds_ratio = bad_embeds/len(is_good_embed)\n",
    "\n",
    "    return bad_embeds, bad_embeds_ratio\n",
    "\n",
    "def train_vectors(n_lst, embeddings, n_steps=100, verbose=False):\n",
    "    min_bad = len(n_lst)\n",
    "    X = embeddings\n",
    "    self_emb = X[n_lst]\n",
    "    mask = torch.ones((len(n_lst), len(X)), requires_grad=False, device=X.device)\n",
    "    indices = torch.arange(len(n_lst))\n",
    "    mask[indices, n_lst] = 0\n",
    "\n",
    "    x_optim = self_emb.detach().clone()\n",
    "    x_optim.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW([x_optim], lr=0.01)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        bad_embeds, bad_embeds_ratio = calc_bad_embeds(x_optim, self_emb, embeddings, mask)\n",
    "        print(f\"Initial\\nloss = {loss.item()}\")\n",
    "        print(f\"Bad embeds = {bad_embeds}/{len(n_lst)}, ratio = {bad_embeds_ratio}\")\n",
    "\n",
    "    pbar = tqdm(range(n_steps))\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (step+1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                bad_embeds, bad_embeds_ratio = calc_bad_embeds(x_optim, self_emb, embeddings, mask)\n",
    "            pbar.set_postfix_str(f\"Bad embeds: {bad_embeds}/{len(n_lst)}\")\n",
    "            if bad_embeds < min_bad:\n",
    "                min_bad = bad_embeds\n",
    "            if bad_embeds_ratio == 0.0:\n",
    "                break\n",
    "            if verbose:\n",
    "                print(f\"Step {step + 1}, Loss: {loss.item()}, bad embeds = {bad_embeds}/{len(n_lst)}, ratio = {bad_embeds_ratio}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = calc_loss(x_optim, self_emb, X, mask)\n",
    "        bad_embeds, bad_embeds_ratio = calc_bad_embeds(x_optim, self_emb, embeddings, mask)\n",
    "    print(\"Final\")\n",
    "    print(f\"steps = {step+1}, loss = {loss.item()}\")\n",
    "    print(f\"Bad embeds = {bad_embeds}/{len(n_lst)}, ratio = {bad_embeds_ratio:.4f}, Minimal bad = {min_bad}\")\n",
    "\n",
    "    return loss, x_optim, self_emb, mask"
   ],
   "id": "56ced80957683661",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:38:45.988786Z",
     "start_time": "2025-03-21T08:38:45.985626Z"
    }
   },
   "cell_type": "code",
   "source": "n_lst = fail_indices",
   "id": "7d8304977669069b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-21T08:38:49.018100Z"
    }
   },
   "cell_type": "code",
   "source": "loss, x_optim, self_emb, mask = train_vectors(n_lst, embeddings, n_steps=100000)",
   "id": "7e0ac16307e2f75c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial\n",
      "loss = 19570.24609375\n",
      "Bad embeds = 1051/1060, ratio = 0.9915094339622641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 230/100000 [00:09<1:12:00, 23.09it/s, Bad embeds: 321/1060]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae3541425728e7fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
