{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ray.rllib.utils.numpy import torch\n",
    "\n",
    "'''\n",
    "Question: if you take any vector from the embedding metric,\n",
    "is it true that its dot-product will be maximal with itself, and with other vectors - less?\n",
    "In other words, if you encode a token into a vector, then is it decoded back using argmax.\n",
    "Obviously, if all vectors are normalized, then yes - the dot product with itself gives 1, and with others - less than 1.\n",
    "But for real embedding matrices this is not the case.\n",
    "In Llama, about 500 out of 128,000 vectors are not decoded back.\n",
    "And basically these are either tokens not from English, or tokens from Python like \\t\\t\\t\\t\\t\\t\\t\\t\\r\\n, or reserved_special_token.\n",
    "If you do the same with the head of the model, there are more such errors - ~800, some long tokens from the English language are added there.\n",
    "It's also interesting that after normalizing everything with a vector,\n",
    "everything fell into place for the input embedding matrix, but head still gave 200 discrepancies,\n",
    "apparently due to a numerical error, if I didn't impose it anywhere.\n",
    "'''"
   ],
   "id": "e3a13979a234cb77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ],
   "id": "46a405929cb65b50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_norms(tensor):\n",
    "\n",
    "    norms = torch.norm(tensor, p=2, dim=1)\n",
    "    print(norms.shape)\n",
    "    average_norm = norms.mean().item()\n",
    "    std_norm = norms.std()\n",
    "    print(f\"Average Norm: {average_norm}\")\n",
    "    print(f\"Standard Deviation of Norms: {std_norm}\")\n",
    "    print(f\"std/mead: {std_norm/average_norm}\")\n",
    "\n",
    "\n",
    "def find_self_embeds(embedding: torch.Tensor, tokenizer, query_matrix: torch.Tensor | None = None) -> tuple[list[int], list[int], list]:\n",
    "\n",
    "    '''\n",
    "    For each vector of embedding we find the closest vector in a matrix.\n",
    "    Ideally it should be the same vector. If not, we count and return number of such discrepancies.\n",
    "    We do it in batches, since all vectors can not fit the GPU\n",
    "    '''\n",
    "\n",
    "    start = 0\n",
    "    end = len(embedding)\n",
    "    all_indices = np.array(range(end))\n",
    "    result_indices = []\n",
    "    batch_size = 10000\n",
    "    batches = [\n",
    "        range(i, min(i + batch_size, end)) for i in range(start, end, batch_size)\n",
    "    ]\n",
    "    eqs = []\n",
    "    if query_matrix is None:\n",
    "        query_matrix = embedding\n",
    "    query_matrix = query_matrix.cuda()\n",
    "    embedding = embedding.cuda()\n",
    "\n",
    "    for batch in tqdm(batches):\n",
    "        # indices = np.array(list(batch))\n",
    "        X = query_matrix[batch]\n",
    "        w = torch.argmax(X @ embedding.T, dim=-1).cpu() #.numpy()\n",
    "        result_indices.extend(w)\n",
    "    result_indices = np.array(result_indices)\n",
    "    eqs = all_indices == result_indices\n",
    "    fail_indices = all_indices[~eqs]\n",
    "    fail_result_indices = result_indices[~eqs]\n",
    "\n",
    "    failed_emb_toks = [tokenizer.decode(idx) for idx in fail_indices]\n",
    "    failed_res_emb_toks = [tokenizer.decode(idx) for idx in fail_result_indices]\n",
    "    failed_pairs = list(zip(failed_emb_toks, failed_res_emb_toks))\n",
    "\n",
    "    return fail_indices, fail_result_indices, failed_pairs\n"
   ],
   "id": "54af067442078ece"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_tokens_from_vectors(embedding_matrix, batch_size, num_batches, do_rms=False, model_norm=None):\n",
    "\n",
    "    input_dim = embedding_matrix.shape[1]\n",
    "    emb_mean = torch.mean(torch.mean(embedding_matrix, dim=0))\n",
    "    emb_std = torch.mean(torch.std(embedding_matrix, dim=0))\n",
    "    torch.manual_seed(1234)\n",
    "    vectors = torch.normal(mean=emb_mean, std=emb_std, size=(num_batches * batch_size, input_dim))\n",
    "\n",
    "    tokens = []\n",
    "    for i in tqdm(range(0, len(vectors), batch_size)):\n",
    "        batch = vectors[i : i + batch_size].cuda()\n",
    "        if do_rms:\n",
    "            batch  = model_norm(batch)\n",
    "        predictions = torch.matmul(batch, embedding_matrix.T)\n",
    "        token_ids = torch.argmax(predictions, dim=1).tolist()\n",
    "        tokens.extend(token_ids)\n",
    "    return tokens\n",
    "\n",
    "def plot_dist(tokens):\n",
    "    token_counts = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_counts.items())\n",
    "    tokens, counts = zip(*sorted_tokens)\n",
    "\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    plt.bar(tokens, counts, width=1.0, edgecolor=\"black\", color='blue')\n",
    "    plt.title(\"Token Distribution\")\n",
    "    plt.xlabel(\"Token Index\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    return token_counts\n",
    "\n",
    "def plot_emb_dist(emb):\n",
    "    # Convert it into a NumPy array for easier plotting\n",
    "    numpy_tensor = emb.detach().cpu().numpy()\n",
    "\n",
    "    # Select a subset of positions to visualize (for simplicity, use every 100th position)\n",
    "    positions_to_plot = range(0, 4096, 10)  # Change step size depending on how dense you want the plot\n",
    "\n",
    "    # Prepare the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot the distributions for selected positions\n",
    "    for pos in positions_to_plot:\n",
    "        sns.kdeplot(numpy_tensor[:, pos], linewidth=1) # label=f\"Position {pos}\",\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title(\"Distribution of Values at Different Positions\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc=\"upper right\", fontsize='small')\n",
    "    plt.show()"
   ],
   "id": "8e626e26fef50680"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_model_and_embed(model_name):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    embedding = model.model.embed_tokens.weight.cuda()  # Embedding layer weights\n",
    "    head = model.lm_head.weight.cuda()\n",
    "    model_norm = model.model.norm.cuda()\n",
    "\n",
    "    emb_norms = torch.norm(embedding, dim=1)\n",
    "    filtered_norms = emb_norms[emb_norms >= 10e-5]\n",
    "    mean_norm = torch.mean(filtered_norms)\n",
    "\n",
    "    return model, embedding, head, model_norm, mean_norm, tokenizer"
   ],
   "id": "c770fae1b0fe5766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loading embeddings:\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"  # \"meta-llama/Meta-Llama-3.1-8B\" , \"gpt2\", \"meta-llama/Llama-2-7b-hf\" # Example: using GPT-2\n",
    "model, embedding, head, model_norm, mean_norm, tokenizer = get_model_and_embed(model_name)\n",
    "\n",
    "embedding_norm = (embedding.T / torch.norm(embedding, p=2, dim=1)).T\n",
    "head_norm = (head.T / torch.norm(head, p=2, dim=1)).T\n",
    "embedding_rms_norm  = model_norm(embedding)\n",
    "head_rms_norm  = model_norm(head)"
   ],
   "id": "a22a0ab69b9af0ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokens_dist = get_tokens_from_vectors(head, batch_size=1000, num_batches=1000, do_rms=False, model_norm=model_norm)",
   "id": "fd04d5adc65e9608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "token_counts = plot_dist(tokens_dist)",
   "id": "85f12263c642deab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "decoded_counts = {tokenizer.decode(token_id): count for token_id, count in token_counts.items()}\n",
    "decoded_counts = dict(sorted(decoded_counts.items(), key=lambda item: item[1], reverse=True))"
   ],
   "id": "821ccf2dd9f9ae04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tok_freqs = list(decoded_counts.values())",
   "id": "eb50211bd9b3d2d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tok_freqs_filt = [freq for freq in tok_freqs if 0 <= freq <= 100]\n",
    "sns.histplot(tok_freqs_filt, bins=300, kde=False, log=True)"
   ],
   "id": "367200a975758d2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "    failed_emb, failed_res_emb, failed_pairs = find_self_embeds(embedding, tokenizer, embedding_rms_norm)",
   "id": "8d8b9863cb09379d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "failed_emb, failed_res_emb, failed_pairs = find_self_embeds(head, tokenizer, head_rms_norm)",
   "id": "51f5b4eed8ace800"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "failed_emb = find_self_embeds(embedding, embedding)\n",
    "print(len(failed_emb))\n",
    "failed_emb_norm = find_self_embeds(embedding_norm, embedding_norm)\n",
    "print(len(failed_emb))\n",
    "\n",
    "failed_head = find_self_embeds(head, head)\n",
    "\n",
    "failed_emb_toks = [tokenizer.decode(idx) for idx in failed_emb]\n",
    "failed_head_toks = [tokenizer.decode(idx) for idx in failed_head]\n",
    "\n",
    "joined_set = set(failed_emb_toks) | set(failed_head_toks)"
   ],
   "id": "e427ee331cc09794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "failed_emb = find_self_embeds(embedding, embedding_norm)",
   "id": "fd49b74582ebf74d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model",
   "id": "62fbb8245dbe005"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "83e79c8037fa372c"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
