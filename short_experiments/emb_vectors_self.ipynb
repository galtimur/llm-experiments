{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Question: if you take any vector from the embedding metric,\n",
    "is it true that its dot-product will be maximal with itself, and with other vectors - less?\n",
    "In other words, if you encode a token into a vector, then is it decoded back using argmax.\n",
    "Obviously, if all vectors are normalized, then yes - the dot product with itself gives 1, and with others - less than 1.\n",
    "But for real embedding matrices this is not the case.\n",
    "In Llama, about 500 out of 128,000 vectors are not decoded back.\n",
    "And basically these are either tokens not from English, or tokens from Python like \\t\\t\\t\\t\\t\\t\\t\\t\\r\\n, or reserved_special_token.\n",
    "If you do the same with the head of the model, there are more such errors - ~800, some long tokens from the English language are added there.\n",
    "It's also interesting that after normalizing everything with a vector,\n",
    "everything fell into place for the input embedding matrix, but head still gave 200 discrepancies,\n",
    "apparently due to a numerical error, if I didn't impose it anywhere.\n",
    "\"\"\""
   ],
   "id": "7c41f48363ab8f11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:46:26.127408Z",
     "start_time": "2025-03-31T09:46:24.143924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "from emb_vectors_functions import find_self_embeds, get_model_and_embed\n",
    "from emb_dist_functions import get_tokens_from_vectors, plot_dist"
   ],
   "id": "87754efcb8228938",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_model_and_embed' from 'emb_vectors_functions' (/home/galimzyanov/llm-experiments/short_experiments/emb_vectors_functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01memb_vectors_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m find_self_embeds, get_model_and_embed\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01memb_dist_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_tokens_from_vectors, plot_dist\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'get_model_and_embed' from 'emb_vectors_functions' (/home/galimzyanov/llm-experiments/short_experiments/emb_vectors_functions.py)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_norms(tensor):\n",
    "\n",
    "    norms = torch.norm(tensor, p=2, dim=1)\n",
    "    print(norms.shape)\n",
    "    average_norm = norms.mean().item()\n",
    "    std_norm = norms.std()\n",
    "    print(f\"Average Norm: {average_norm}\")\n",
    "    print(f\"Standard Deviation of Norms: {std_norm}\")\n",
    "    print(f\"std/mead: {std_norm/average_norm}\")"
   ],
   "id": "269aff29ec2964aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loading embeddings:\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"  # \"meta-llama/Meta-Llama-3.1-8B\" , \"gpt2\", \"meta-llama/Llama-2-7b-hf\"\n",
    "model, embedding, head, model_norm, mean_norm, tokenizer = get_model_and_embed(\n",
    "    model_name\n",
    ")\n",
    "\n",
    "embedding_norm = (embedding.T / torch.norm(embedding, p=2, dim=1)).T\n",
    "head_norm = (head.T / torch.norm(head, p=2, dim=1)).T\n",
    "embedding_rms_norm = model_norm(embedding)\n",
    "head_rms_norm = model_norm(head)"
   ],
   "id": "36948eb932804f3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokens_dist = get_tokens_from_vectors(\n",
    "    head, batch_size=1000, num_batches=1000, do_rms=False, model_norm=model_norm\n",
    ")"
   ],
   "id": "cb276a48d8877948"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "token_counts = plot_dist(tokens_dist)",
   "id": "85f12263c642deab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "decoded_counts = {\n",
    "    tokenizer.decode(token_id): count for token_id, count in token_counts.items()\n",
    "}\n",
    "decoded_counts = dict(\n",
    "    sorted(decoded_counts.items(), key=lambda item: item[1], reverse=True)\n",
    ")"
   ],
   "id": "821ccf2dd9f9ae04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tok_freqs = list(decoded_counts.values())",
   "id": "eb50211bd9b3d2d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tok_freqs_filt = [freq for freq in tok_freqs if 0 <= freq <= 100]\n",
    "sns.histplot(tok_freqs_filt, bins=300, kde=False, log=True)"
   ],
   "id": "b696df853046028c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "failed_emb, failed_res_emb, failed_pairs = find_self_embeds(\n",
    "    head, tokenizer, head_rms_norm\n",
    ")"
   ],
   "id": "345a785118534e8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "failed_emb = find_self_embeds(embedding, embedding)\n",
    "print(len(failed_emb))\n",
    "failed_emb_norm = find_self_embeds(embedding_norm, embedding_norm)\n",
    "print(len(failed_emb))\n",
    "\n",
    "failed_head = find_self_embeds(head, head)\n",
    "\n",
    "failed_emb_toks = [tokenizer.decode(idx) for idx in failed_emb]\n",
    "failed_head_toks = [tokenizer.decode(idx) for idx in failed_head]\n",
    "\n",
    "joined_set = set(failed_emb_toks) | set(failed_head_toks)"
   ],
   "id": "45420bdf2842e4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "failed_emb = find_self_embeds(embedding, embedding_norm)",
   "id": "fa91a5d763567147"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "459c5373ca21ba9f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
